# üî¨ Deep Research con API Local y DuckDuckGo

Este README explica c√≥mo configurar y usar el sistema Deep Research con tu API local (LM Studio) y DuckDuckGo como motor de b√∫squeda, sin necesidad de APIs externas costosas.

## üöÄ Caracter√≠sticas

- ‚úÖ **API Local**: Usa tu modelo local (LM Studio) en lugar de APIs externas
- ‚úÖ **B√∫squeda Gratuita**: DuckDuckGo como motor de b√∫squeda sin costos
- ‚úÖ **Sin Autenticaci√≥n**: Configuraci√≥n simplificada para desarrollo local
- ‚úÖ **Investigaci√≥n Profunda**: Sistema completo de investigaci√≥n automatizada
- ‚úÖ **Interfaz Web**: LangGraph Studio para interacci√≥n f√°cil

## üìã Requisitos Previos

### 1. LM Studio
- Instala [LM Studio](https://lmstudio.ai/)
- Carga un modelo compatible (ej: `openai/gpt-oss-20b`)
- Inicia el servidor en `localhost:1234`

### 2. Python y Dependencias
```bash
# Instalar dependencias
uv sync
# o
pip install -r requirements.txt
```

## ‚öôÔ∏è Configuraci√≥n

### 1. Archivo de Configuraci√≥n

El archivo `config_local.env` contiene toda la configuraci√≥n necesaria:

```env
# Configuraci√≥n del modelo local (localhost:1234)
RESEARCH_MODEL=openai:http://localhost:1234/v1
SUMMARIZATION_MODEL=openai:http://localhost:1234/v1
COMPRESSION_MODEL=openai:http://localhost:1234/v1
FINAL_REPORT_MODEL=openai:http://localhost:1234/v1

# API Key para el modelo local - usar cualquier valor ya que tu API acepta cualquier clave
OPENAI_API_KEY=test-key

# Configuraci√≥n de b√∫squeda - usar DuckDuckGo en lugar de Tavily
SEARCH_API=duckduckgo

# Configuraci√≥n de tokens
RESEARCH_MODEL_MAX_TOKENS=10000
SUMMARIZATION_MODEL_MAX_TOKENS=8192
COMPRESSION_MODEL_MAX_TOKENS=8192
FINAL_REPORT_MODEL_MAX_TOKENS=10000

# Configuraci√≥n de investigaci√≥n
MAX_CONCURRENT_RESEARCH_UNITS=3
MAX_RESEARCHER_ITERATIONS=4
MAX_REACT_TOOL_CALLS=8
MAX_STRUCTURED_OUTPUT_RETRIES=3

# Configuraci√≥n de contenido
MAX_CONTENT_LENGTH=50000

# Configuraci√≥n de clarificaci√≥n
ALLOW_CLARIFICATION=true
```

### 2. Verificar API Local

Antes de usar el sistema, verifica que tu API local est√© funcionando:

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      {"role": "user", "content": "Hello"}
    ],
    "temperature": 0.7,
    "max_tokens": 10,
    "stream": false
}'
```

## üöÄ Uso

### Opci√≥n 1: Script de Inicio Autom√°tico

```bash
# Hacer ejecutable
chmod +x start_local_dev.sh

# Ejecutar
./start_local_dev.sh
```

### Opci√≥n 2: Comando Manual

```bash
# Cargar configuraci√≥n
export $(grep -v '^#' config_local.env | xargs)

# Iniciar servidor
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking --auth ./src/security/auth_local.py:auth
```

### Opci√≥n 3: Sin Autenticaci√≥n (Recomendado para desarrollo)

```bash
# Usar autenticaci√≥n local simplificada
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking --auth ./src/security/auth_local.py:auth
```

## üåê Acceso a la Interfaz

Una vez iniciado el servidor:

- **API**: http://127.0.0.1:2024
- **Studio UI**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- **API Docs**: http://127.0.0.1:2024/docs

### Configuraci√≥n en la Interfaz

1. Abre el Studio UI
2. Ve a "Manage Assistants"
3. Configura:
   - **Search API**: `DuckDuckGo`
   - **Research Model**: `openai:http://localhost:1234/v1`
   - **Summarization Model**: `openai:http://localhost:1234/v1`
   - **Compression Model**: `openai:http://localhost:1234/v1`
   - **Final Report Model**: `openai:http://localhost:1234/v1`

## üß™ Pruebas

### Script de Prueba Autom√°tico

```bash
python3 test_local_config.py
```

Este script verifica:
- ‚úÖ Conexi√≥n a API local
- ‚úÖ Funcionamiento de DuckDuckGo
- ‚úÖ Investigaci√≥n completa end-to-end

### Prueba Manual

```python
import asyncio
from open_deep_research.deep_researcher import deep_researcher

async def test_research():
    config = {
        "configurable": {
            "research_model": "openai:http://localhost:1234/v1",
            "summarization_model": "openai:http://localhost:1234/v1",
            "compression_model": "openai:http://localhost:1234/v1",
            "final_report_model": "openai:http://localhost:1234/v1",
            "search_api": "duckduckgo",
            "max_concurrent_research_units": 3,
            "max_researcher_iterations": 4,
            "max_react_tool_calls": 8,
            "max_structured_output_retries": 3,
            "max_content_length": 50000,
            "allow_clarification": True
        }
    }
    
    result = await deep_researcher.ainvoke(
        {"messages": [{"role": "user", "content": "¬øCu√°les son las √∫ltimas noticias sobre IA?"}]},
        config=config
    )
    
    print(result["final_report"])

# Ejecutar
asyncio.run(test_research())
```

## üîß Personalizaci√≥n

### Cambiar Modelo Local

Edita `config_local.env`:

```env
# Cambiar a otro modelo
RESEARCH_MODEL=openai:http://localhost:1234/v1
# El modelo real se especifica en la URL de tu API local
```

### Ajustar Par√°metros de Investigaci√≥n

```env
# M√°s unidades de investigaci√≥n concurrentes (m√°s r√°pido, m√°s recursos)
MAX_CONCURRENT_RESEARCH_UNITS=5

# M√°s iteraciones de investigaci√≥n (m√°s profundo)
MAX_RESEARCHER_ITERATIONS=6

# M√°s llamadas de herramientas por iteraci√≥n
MAX_REACT_TOOL_CALLS=10
```

### Cambiar Motor de B√∫squeda

```env
# Usar Tavily (requiere API key)
SEARCH_API=tavily

# Usar b√∫squeda nativa de OpenAI
SEARCH_API=openai

# Usar b√∫squeda nativa de Anthropic
SEARCH_API=anthropic

# Sin b√∫squeda (solo modelo local)
SEARCH_API=none
```

## üêõ Soluci√≥n de Problemas

### Error: "Authorization header missing"

**Problema**: El servidor requiere autenticaci√≥n.

**Soluci√≥n**: Usa el script con autenticaci√≥n local:
```bash
./start_local_dev.sh
```

### Error: "Incorrect API key provided"

**Problema**: Tu API local rechaza la clave API.

**Soluci√≥n**: Verifica que tu API local no requiera autenticaci√≥n:
```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "openai/gpt-oss-20b", "messages": [{"role": "user", "content": "test"}]}'
```

### Error: "No tools found to conduct research"

**Problema**: DuckDuckGo no est√° configurado correctamente.

**Soluci√≥n**: Verifica la configuraci√≥n:
```env
SEARCH_API=duckduckgo
```

### Error: "Connection refused"

**Problema**: LM Studio no est√° ejecut√°ndose.

**Soluci√≥n**: 
1. Abre LM Studio
2. Carga un modelo
3. Inicia el servidor en puerto 1234

## üìä Rendimiento

### Recursos del Sistema

- **RAM**: ~12-15 GB (dependiendo del modelo)
- **GPU**: Recomendado para modelos grandes
- **CPU**: M√≠nimo 8 cores para buen rendimiento

### Optimizaciones

```env
# Reducir concurrencia para sistemas con menos recursos
MAX_CONCURRENT_RESEARCH_UNITS=2

# Reducir tokens para ahorrar memoria
RESEARCH_MODEL_MAX_TOKENS=5000
SUMMARIZATION_MODEL_MAX_TOKENS=4000
```

## üîí Seguridad

### Desarrollo Local

- ‚úÖ No se env√≠an datos a servicios externos
- ‚úÖ B√∫squedas a trav√©s de DuckDuckGo (sin tracking)
- ‚úÖ Modelo ejecut√°ndose localmente

### Producci√≥n

Para uso en producci√≥n, considera:
- Configurar autenticaci√≥n real
- Usar HTTPS
- Implementar rate limiting
- Monitoreo de logs

## üìö Ejemplos de Uso

### Investigaci√≥n de Noticias

```python
pregunta = "¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial en 2024?"
```

### An√°lisis de Mercado

```python
pregunta = "¬øCu√°l es el estado actual del mercado de criptomonedas?"
```

### Investigaci√≥n Cient√≠fica

```python
pregunta = "¬øCu√°les son los √∫ltimos avances en computaci√≥n cu√°ntica?"
```

## ü§ù Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature
3. Haz commit de tus cambios
4. Push a la rama
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver `LICENSE` para m√°s detalles.

## üôè Agradecimientos

- [LangChain](https://langchain.com/) - Framework de IA
- [LangGraph](https://langchain.com/langgraph) - Grafo de agentes
- [LM Studio](https://lmstudio.ai/) - Servidor local de modelos
- [DuckDuckGo](https://duckduckgo.com/) - Motor de b√∫squeda privado

---

**¬°Disfruta investigando con tu propio sistema de IA local!** üöÄ
